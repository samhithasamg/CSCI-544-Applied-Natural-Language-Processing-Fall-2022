{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samhitha Gundam\n",
    "### HW 2 -\n",
    "**Question 1,2,3,4 are present in this notebook and Q5 is present in the second Notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/samhitha/opt/anaconda3/lib/python3.9/site-packages (1.12.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/samhitha/opt/anaconda3/lib/python3.9/site-packages (from torch) (4.1.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uc7xtMaaqaY3",
    "outputId": "6d9f0cc1-2d8b-4d5c-de62-f8efc09d6db5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/samhitha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim.models\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02K4pn05qlJe",
    "outputId": "9b0c6e06-185d-4ade-8aea-893393093bed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/dvx5c3n50kx5fh6qcgyy221w0000gn/T/ipykernel_3452/4051224240.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_data=pd.read_csv(\"amazon_reviews_us_Jewelry_v1_00.tsv\",sep=\"\\t\",on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "raw_data=pd.read_csv(\"amazon_reviews_us_Jewelry_v1_00.tsv\",sep=\"\\t\",on_bad_lines='skip')\n",
    "data=raw_data[['star_rating','review_body']]\n",
    "data=data.dropna()\n",
    "data = data.reset_index(drop=True)\n",
    "data['star_rating']=data['star_rating'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "G3XFOu27wWWX",
    "outputId": "03ec01c8-0612-4c98-8ca0-8b3d08ac9ebf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>so beautiful even tho clearly not high end ......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.. I got this set for my mother, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Love it. Fits great. Super comfortable and nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Got this as a Mother's Day gift for my Mom and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766743</th>\n",
       "      <td>4</td>\n",
       "      <td>It is nice looking and everything (it is sterl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766744</th>\n",
       "      <td>4</td>\n",
       "      <td>my boyfriend bought me this last christmas, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766745</th>\n",
       "      <td>4</td>\n",
       "      <td>This is a great way to quickly start learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766746</th>\n",
       "      <td>5</td>\n",
       "      <td>the 14kt gold earrings look remarkable...would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766747</th>\n",
       "      <td>5</td>\n",
       "      <td>It will be a gift to my special friend. We kno...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1766748 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "0                  5  so beautiful even tho clearly not high end ......\n",
       "1                  5  Great product.. I got this set for my mother, ...\n",
       "2                  5  Exactly as pictured and my daughter's friend l...\n",
       "3                  5  Love it. Fits great. Super comfortable and nea...\n",
       "4                  5  Got this as a Mother's Day gift for my Mom and...\n",
       "...              ...                                                ...\n",
       "1766743            4  It is nice looking and everything (it is sterl...\n",
       "1766744            4  my boyfriend bought me this last christmas, an...\n",
       "1766745            4  This is a great way to quickly start learning ...\n",
       "1766746            5  the 14kt gold earrings look remarkable...would...\n",
       "1766747            5  It will be a gift to my special friend. We kno...\n",
       "\n",
       "[1766748 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YU_y_6lZqlMI"
   },
   "outputs": [],
   "source": [
    "df=data.groupby('star_rating').sample(n=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HuNb5dsjqlOK",
    "outputId": "93e3ca37-e140-420c-ccc9-5af5049ee858"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/dvx5c3n50kx5fh6qcgyy221w0000gn/T/ipykernel_3452/77415047.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['review_body']=df['review_body'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
      "/var/folders/3c/dvx5c3n50kx5fh6qcgyy221w0000gn/T/ipykernel_3452/77415047.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['review_body']=df['review_body'].str.replace('[^a-zA-Z0-9 ]', '')\n",
      "/var/folders/3c/dvx5c3n50kx5fh6qcgyy221w0000gn/T/ipykernel_3452/77415047.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['review_body']=df['review_body'].str.replace('/ +/', ' ')#convert multispace to space\n",
      "/var/folders/3c/dvx5c3n50kx5fh6qcgyy221w0000gn/T/ipykernel_3452/77415047.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['review_body']=df['review_body'].str.replace('/^ /', '') # remove spaces in the start of the string\n",
      "/var/folders/3c/dvx5c3n50kx5fh6qcgyy221w0000gn/T/ipykernel_3452/77415047.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['review_body']=df['review_body'].str.replace('/ $/', '') # remove unnecesary space at the end of the string\n"
     ]
    }
   ],
   "source": [
    "df['review_body']=df['review_body'].str.lower()  #convert to lower case\n",
    "\n",
    "#remove contractions\n",
    "df['review_body']=df['review_body'].str.replace(\"\\'re\",\" are\")\n",
    "df['review_body']=df['review_body'].str.replace(\"br\",\" \")\n",
    "df['review_body']=df['review_body'].str.replace(\"\\n't\",\" not\")\n",
    "df['review_body']=df['review_body'].str.replace(\"\\'s'\",\" is\")\n",
    "df['review_body']=df['review_body'].str.replace(\"i'm\",\" i am\")\n",
    "df['review_body']=df['review_body'].str.replace(\"\\'ve'\",\" have\")\n",
    "df['review_body']=df['review_body'].str.replace(\"din't\",\"did not\")\n",
    "\n",
    "df['review_body']=df['review_body'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "df['review_body']=df['review_body'].str.replace('[^a-zA-Z0-9 ]', '')\n",
    "df['review_body']=df['review_body'].str.replace('/ +/', ' ')#convert multispace to space\n",
    "df['review_body']=df['review_body'].str.replace('/^ /', '') # remove spaces in the start of the string\n",
    "df['review_body']=df['review_body'].str.replace('/ $/', '') # remove unnecesary space at the end of the string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ttqvoeh2y7VA"
   },
   "source": [
    "**Question 2(a)** Implementation of word2Vec using google new data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HlSSTBszy3lk",
    "outputId": "7d29151f-0d97-49a7-f1d7-48141e4cbee3"
   },
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674735069275),\n",
       " ('princess', 0.5902431011199951)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive = ['king','woman'], negative =['man'],topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mother', 0.8462508320808411),\n",
       " ('daughter', 0.7899606227874756),\n",
       " ('husband', 0.7560456991195679)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive = ['father','woman'], negative =['man'],topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Zvl4ywo2yvFL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excellent outstanding 0.5567486\n"
     ]
    }
   ],
   "source": [
    "print('excellent', 'outstanding',wv.similarity('excellent','outstanding'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good great 0.72915095\n",
      "worst bad 0.43674564\n",
      "car truck 0.67357904\n"
     ]
    }
   ],
   "source": [
    "print('good', 'great',wv.similarity('good','great'))\n",
    "print('worst','bad',wv.similarity('worst','bad'))\n",
    "print('car', 'truck',wv.similarity('car','truck'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDek12oiAnls"
   },
   "source": [
    "**Question 2b** - Word2Vec using review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9TunN6w1qlQu"
   },
   "outputs": [],
   "source": [
    "X=df['review_body']\n",
    "Y=df['star_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1QbfHM5lx5S-"
   },
   "outputs": [],
   "source": [
    "inputs=X.to_list()\n",
    "sentences=[k.split() for k in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "pc-QTXBKx6SY"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=sentences,vector_size=300, window=11, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('avenue', 0.5949714779853821),\n",
       " ('candy', 0.5309351086616516),\n",
       " ('amazoncom', 0.5016636252403259)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive = ['king','woman'], negative =['man'],topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('grandchildren', 0.6483133435249329),\n",
       " ('lady', 0.6390446424484253),\n",
       " ('cousin', 0.6286104321479797)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive = ['father','woman'], negative =['man'],topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excellent outstanding 0.70434016\n",
      "good great 0.83079803\n",
      "worst bad 0.83079803\n"
     ]
    }
   ],
   "source": [
    "print('excellent', 'outstanding',model.wv.similarity('excellent','outstanding'))\n",
    "print('good', 'great',model.wv.similarity('good','great'))\n",
    "print('worst','bad',model.wv.similarity('good','great'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the above values of similarity we can state that pretrained google model is better for terms like king, queen but for case of adjectives like good, bad, excellent which are greatly a part of the review dataset, our word2vec model performs better as there are greater examples with such used thus allowing the model to be trained better**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msL1YSzS5WfK"
   },
   "source": [
    "**Question 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "eai2SOgM-5de"
   },
   "outputs": [],
   "source": [
    "X=df['review_body']\n",
    "Y=df['star_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ssuEUBuayRuU"
   },
   "outputs": [],
   "source": [
    "X=pd.DataFrame(X)\n",
    "sentences=[k.split() for k in X['review_body']]\n",
    "X['reviews_split']=sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aN1YpXgBCDCD"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>reviews_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1087812</th>\n",
       "      <td>this tarnished it is not sterling silver and t...</td>\n",
       "      <td>[this, tarnished, it, is, not, sterling, silve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220841</th>\n",
       "      <td>got one for the most beautiful woman ive ever ...</td>\n",
       "      <td>[got, one, for, the, most, beautiful, woman, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271914</th>\n",
       "      <td>when i received this ring i was so disappointe...</td>\n",
       "      <td>[when, i, received, this, ring, i, was, so, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387606</th>\n",
       "      <td>pretty but damage</td>\n",
       "      <td>[pretty, but, damage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349019</th>\n",
       "      <td>the enamel on the ring scratches very easily a...</td>\n",
       "      <td>[the, enamel, on, the, ring, scratches, very, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531270</th>\n",
       "      <td>i do not wear silver and very few 34dangle34 e...</td>\n",
       "      <td>[i, do, not, wear, silver, and, very, few, 34d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176086</th>\n",
       "      <td>100 percent satisfied excellent product and ha...</td>\n",
       "      <td>[100, percent, satisfied, excellent, product, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381040</th>\n",
       "      <td>the picture doesnt do this beautiful  acelet j...</td>\n",
       "      <td>[the, picture, doesnt, do, this, beautiful, ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468354</th>\n",
       "      <td>this was a gift for my wife and she loved it</td>\n",
       "      <td>[this, was, a, gift, for, my, wife, and, she, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008475</th>\n",
       "      <td>i ordered this ring for my daughter and she ab...</td>\n",
       "      <td>[i, ordered, this, ring, for, my, daughter, an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body  \\\n",
       "1087812  this tarnished it is not sterling silver and t...   \n",
       "220841   got one for the most beautiful woman ive ever ...   \n",
       "1271914  when i received this ring i was so disappointe...   \n",
       "387606                                  pretty but damage    \n",
       "349019   the enamel on the ring scratches very easily a...   \n",
       "...                                                    ...   \n",
       "1531270  i do not wear silver and very few 34dangle34 e...   \n",
       "176086   100 percent satisfied excellent product and ha...   \n",
       "1381040  the picture doesnt do this beautiful  acelet j...   \n",
       "468354        this was a gift for my wife and she loved it   \n",
       "1008475  i ordered this ring for my daughter and she ab...   \n",
       "\n",
       "                                             reviews_split  \n",
       "1087812  [this, tarnished, it, is, not, sterling, silve...  \n",
       "220841   [got, one, for, the, most, beautiful, woman, i...  \n",
       "1271914  [when, i, received, this, ring, i, was, so, di...  \n",
       "387606                               [pretty, but, damage]  \n",
       "349019   [the, enamel, on, the, ring, scratches, very, ...  \n",
       "...                                                    ...  \n",
       "1531270  [i, do, not, wear, silver, and, very, few, 34d...  \n",
       "176086   [100, percent, satisfied, excellent, product, ...  \n",
       "1381040  [the, picture, doesnt, do, this, beautiful, ac...  \n",
       "468354   [this, was, a, gift, for, my, wife, and, she, ...  \n",
       "1008475  [i, ordered, this, ring, for, my, daughter, an...  \n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VzMhQiT7yTWr",
    "outputId": "cc79f38f-c1ff-42c7-ac1e-47a25bcf3b40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/dvx5c3n50kx5fh6qcgyy221w0000gn/T/ipykernel_71284/2766180496.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  features.append(vector/count)\n"
     ]
    }
   ],
   "source": [
    "features=[]\n",
    "for lis in X['reviews_split']:\n",
    "    vector=np.zeros(300,)\n",
    "    count=len(lis)\n",
    "    for j in lis:\n",
    "        if j in wv.key_to_index:\n",
    "            vector=vector+wv[j]\n",
    "    features.append(vector/count)\n",
    "    \n",
    "X['input']=features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EIh-HhQJzDe1"
   },
   "outputs": [],
   "source": [
    "df3 = X.input.apply(pd.Series)\n",
    "df3=df3.replace(np.nan,0)\n",
    "X_train,X_test,Y_train, Y_test = train_test_split(df3,Y, test_size=0.2, random_state=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron model - accuracy obtained with TF-IDF features - 41%; Word2Vec - 33% . In case of perceptron model, the TF-IDF features perform better than word2Vec features but not by much**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmXo5BjTydJ4",
    "outputId": "d049d8b4-651f-4cd9-d8a1-14f71ec4919d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "clf_perceptron = Perceptron()\n",
    "clf_perceptron.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JLtjlBNgPTHI"
   },
   "outputs": [],
   "source": [
    "y_test_perceptron=clf_perceptron.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "sYXJc3uwPWbs"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report=classification_report(Y_test, y_test_perceptron,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.614221916867056,\n",
       "  'recall': 0.4452191235059761,\n",
       "  'f1-score': 0.5162407968817669,\n",
       "  'support': 4016},\n",
       " '2': {'precision': 0.37142857142857144,\n",
       "  'recall': 0.07454500124657193,\n",
       "  'f1-score': 0.12416943521594685,\n",
       "  'support': 4011},\n",
       " '3': {'precision': 0.31971153846153844,\n",
       "  'recall': 0.09922904750062174,\n",
       "  'f1-score': 0.1514518884038717,\n",
       "  'support': 4021},\n",
       " '4': {'precision': 0.25334829254610475,\n",
       "  'recall': 0.8992035838725734,\n",
       "  'f1-score': 0.39531703047212646,\n",
       "  'support': 4018},\n",
       " '5': {'precision': 0.7212903225806452,\n",
       "  'recall': 0.14209456024402645,\n",
       "  'f1-score': 0.23741771076661713,\n",
       "  'support': 3934},\n",
       " 'accuracy': 0.3329,\n",
       " 'macro avg': {'precision': 0.4560001283767832,\n",
       "  'recall': 0.3320582632739539,\n",
       "  'f1-score': 0.2849193723480658,\n",
       "  'support': 20000},\n",
       " 'weighted avg': {'precision': 0.45487924413872244,\n",
       "  'recall': 0.3329,\n",
       "  'f1-score': 0.2851319895396592,\n",
       "  'support': 20000}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM model - accuracy obtained with TF-IDF features - 49%; Word2Vec - 48%. Similar to case of perceptron model, In case of SVM model too, the TF-IDF features perform better than word2Vec features but by a very marginal difference .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Oxit0q7YPcU4"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf_SVM = LinearSVC()\n",
    "clf_SVM.fit(X_train, Y_train)\n",
    "y_test_SVM=clf_SVM.predict(X_test)\n",
    "report_SVM=classification_report(Y_test, y_test_SVM,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.5076977526101575,\n",
       "  'recall': 0.7143924302788844,\n",
       "  'f1-score': 0.5935657391124445,\n",
       "  'support': 4016},\n",
       " '2': {'precision': 0.38315318673127097,\n",
       "  'recall': 0.256295188232361,\n",
       "  'f1-score': 0.30714072303555423,\n",
       "  'support': 4011},\n",
       " '3': {'precision': 0.40065952184666115,\n",
       "  'recall': 0.36259636906242226,\n",
       "  'f1-score': 0.3806788511749347,\n",
       "  'support': 4021},\n",
       " '4': {'precision': 0.43756558237145854,\n",
       "  'recall': 0.3113489298158288,\n",
       "  'f1-score': 0.36382143376472303,\n",
       "  'support': 4018},\n",
       " '5': {'precision': 0.5688854489164087,\n",
       "  'recall': 0.7473309608540926,\n",
       "  'f1-score': 0.6460118655240605,\n",
       "  'support': 3934},\n",
       " 'accuracy': 0.4773,\n",
       " 'macro avg': {'precision': 0.45959229849519134,\n",
       "  'recall': 0.4783927756487178,\n",
       "  'f1-score': 0.4582437225223434,\n",
       "  'support': 20000},\n",
       " 'weighted avg': {'precision': 0.45914637049063084,\n",
       "  'recall': 0.4773,\n",
       "  'f1-score': 0.4574828154391955,\n",
       "  'support': 20000}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De5Cgu8_QCxy"
   },
   "source": [
    "**Question 4a**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating tensor data\n",
    "#training data\n",
    "xtrain_np=X_train.to_numpy()\n",
    "x=torch.from_numpy(xtrain_np)\n",
    "\n",
    "ytrain_np=Y_train.to_numpy()\n",
    "y=torch.from_numpy(ytrain_np-1)\n",
    "\n",
    "#testing data\n",
    "xtest_np=X_test.to_numpy()\n",
    "test_x=torch.from_numpy(xtest_np)\n",
    "\n",
    "ytest_np=Y_test.to_numpy()\n",
    "test_y=torch.from_numpy(ytest_np-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.nn.Sequential(\n",
    "    torch.nn.Linear(300,50),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(50,10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10,5)    \n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.2\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " traning Accuracy for 0 - tensor(0.2008)\n",
      " traning Accuracy for 50 - tensor(0.2024)\n",
      " traning Accuracy for 100 - tensor(0.2779)\n",
      " traning Accuracy for 150 - tensor(0.3130)\n",
      " traning Accuracy for 200 - tensor(0.3367)\n",
      " traning Accuracy for 250 - tensor(0.3516)\n",
      " traning Accuracy for 300 - tensor(0.3626)\n",
      " traning Accuracy for 350 - tensor(0.3648)\n",
      " traning Accuracy for 400 - tensor(0.3715)\n",
      " traning Accuracy for 450 - tensor(0.3758)\n",
      " traning Accuracy for 500 - tensor(0.3847)\n",
      " traning Accuracy for 550 - tensor(0.3401)\n",
      " traning Accuracy for 600 - tensor(0.3665)\n",
      " traning Accuracy for 650 - tensor(0.3733)\n",
      " traning Accuracy for 700 - tensor(0.3796)\n",
      " traning Accuracy for 750 - tensor(0.3848)\n",
      " traning Accuracy for 800 - tensor(0.3900)\n",
      " traning Accuracy for 850 - tensor(0.3947)\n",
      " traning Accuracy for 900 - tensor(0.3992)\n",
      " traning Accuracy for 950 - tensor(0.4046)\n"
     ]
    }
   ],
   "source": [
    "model=model.float()\n",
    "for t in range(1000):\n",
    "    y_pred = model(x.float())\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if(t%50==0):\n",
    "        pred=model(x.float())\n",
    "        _,predicted = torch.max(pred.data,1)\n",
    "        correct = (predicted == y).sum()\n",
    "        print(\" traning Accuracy for\",t,\"-\",correct/len(y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for question 4a: tensor(0.3986)\n"
     ]
    }
   ],
   "source": [
    "pred=model(test_x.float())\n",
    "_,predicted = torch.max(pred.data,1)\n",
    "correct = (predicted == test_y).sum()\n",
    "print(\"Accuracy for question 4a:\",correct/len(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxwbZPrBbLvO"
   },
   "source": [
    "**Question 4b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "X6YUC1AUbKwc"
   },
   "outputs": [],
   "source": [
    "#Creating feature vector\n",
    "\n",
    "features=[]\n",
    "for lis in X['reviews_split']:\n",
    "    wordvecs=[]\n",
    "    vector=np.zeros(300,)\n",
    "    for j in range(len(lis)):\n",
    "        if(j>=10):\n",
    "            break\n",
    "        else:\n",
    "            if lis[j] in wv.key_to_index:\n",
    "                wordvecs.append(wv[lis[j]])\n",
    "    while len(wordvecs)<10:\n",
    "        wordvecs.append(np.zeros(300,))\n",
    "        \n",
    "    features.append(wordvecs)\n",
    "    \n",
    "X['input_2']=features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1087812</th>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>-0.031738</td>\n",
       "      <td>0.166016</td>\n",
       "      <td>-0.071289</td>\n",
       "      <td>0.015869</td>\n",
       "      <td>-0.003113</td>\n",
       "      <td>-0.084961</td>\n",
       "      <td>-0.048584</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220841</th>\n",
       "      <td>0.062012</td>\n",
       "      <td>0.108398</td>\n",
       "      <td>-0.096680</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.033936</td>\n",
       "      <td>-0.347656</td>\n",
       "      <td>-0.069824</td>\n",
       "      <td>-0.052490</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>0.191406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.203125</td>\n",
       "      <td>-0.010986</td>\n",
       "      <td>-0.122559</td>\n",
       "      <td>0.157227</td>\n",
       "      <td>-0.045898</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>-0.060547</td>\n",
       "      <td>-0.174805</td>\n",
       "      <td>-0.023071</td>\n",
       "      <td>-0.043457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271914</th>\n",
       "      <td>0.170898</td>\n",
       "      <td>0.024292</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>0.022217</td>\n",
       "      <td>0.068848</td>\n",
       "      <td>-0.090820</td>\n",
       "      <td>-0.031738</td>\n",
       "      <td>-0.133789</td>\n",
       "      <td>0.178711</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075684</td>\n",
       "      <td>0.225586</td>\n",
       "      <td>-0.111328</td>\n",
       "      <td>-0.039551</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>-0.113281</td>\n",
       "      <td>-0.176758</td>\n",
       "      <td>0.028198</td>\n",
       "      <td>-0.177734</td>\n",
       "      <td>-0.006042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387606</th>\n",
       "      <td>0.123047</td>\n",
       "      <td>-0.046143</td>\n",
       "      <td>-0.202148</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>-0.027100</td>\n",
       "      <td>-0.037598</td>\n",
       "      <td>-0.103027</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>0.194336</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349019</th>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.104980</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.053467</td>\n",
       "      <td>-0.067383</td>\n",
       "      <td>-0.120605</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>-0.118652</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531270</th>\n",
       "      <td>-0.225586</td>\n",
       "      <td>-0.019531</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0.237305</td>\n",
       "      <td>-0.029297</td>\n",
       "      <td>0.093262</td>\n",
       "      <td>-0.058838</td>\n",
       "      <td>-0.041016</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>0.020020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176086</th>\n",
       "      <td>0.081055</td>\n",
       "      <td>-0.235352</td>\n",
       "      <td>-0.045898</td>\n",
       "      <td>-0.036377</td>\n",
       "      <td>-0.063477</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>0.030396</td>\n",
       "      <td>-0.255859</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>-0.049561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381040</th>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.104980</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.053467</td>\n",
       "      <td>-0.067383</td>\n",
       "      <td>-0.120605</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>-0.118652</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468354</th>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>-0.031738</td>\n",
       "      <td>0.166016</td>\n",
       "      <td>-0.071289</td>\n",
       "      <td>0.015869</td>\n",
       "      <td>-0.003113</td>\n",
       "      <td>-0.084961</td>\n",
       "      <td>-0.048584</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008475</th>\n",
       "      <td>-0.225586</td>\n",
       "      <td>-0.019531</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0.237305</td>\n",
       "      <td>-0.029297</td>\n",
       "      <td>0.093262</td>\n",
       "      <td>-0.058838</td>\n",
       "      <td>-0.041016</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>0.020020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6    \\\n",
       "1087812  0.109375  0.140625 -0.031738  0.166016 -0.071289  0.015869 -0.003113   \n",
       "220841   0.062012  0.108398 -0.096680  0.079102  0.033936 -0.347656 -0.069824   \n",
       "1271914  0.170898  0.024292  0.138672  0.022217  0.068848 -0.090820 -0.031738   \n",
       "387606   0.123047 -0.046143 -0.202148  0.144531 -0.027100 -0.037598 -0.103027   \n",
       "349019   0.080078  0.104980  0.049805  0.053467 -0.067383 -0.120605  0.035156   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1531270 -0.225586 -0.019531  0.090820  0.237305 -0.029297  0.093262 -0.058838   \n",
       "176086   0.081055 -0.235352 -0.045898 -0.036377 -0.063477 -0.130859  0.030396   \n",
       "1381040  0.080078  0.104980  0.049805  0.053467 -0.067383 -0.120605  0.035156   \n",
       "468354   0.109375  0.140625 -0.031738  0.166016 -0.071289  0.015869 -0.003113   \n",
       "1008475 -0.225586 -0.019531  0.090820  0.237305 -0.029297  0.093262 -0.058838   \n",
       "\n",
       "              7         8         9    ...       290       291       292  \\\n",
       "1087812 -0.084961 -0.048584  0.055664  ...  0.000000  0.000000  0.000000   \n",
       "220841  -0.052490 -0.132812  0.191406  ... -0.203125 -0.010986 -0.122559   \n",
       "1271914 -0.133789  0.178711  0.156250  ...  0.075684  0.225586 -0.111328   \n",
       "387606  -0.075195  0.194336  0.138672  ...  0.000000  0.000000  0.000000   \n",
       "349019  -0.118652  0.043945  0.030151  ...  0.000000  0.000000  0.000000   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "1531270 -0.041016  0.052246  0.020020  ...  0.000000  0.000000  0.000000   \n",
       "176086  -0.255859  0.484375 -0.049561  ...  0.000000  0.000000  0.000000   \n",
       "1381040 -0.118652  0.043945  0.030151  ...  0.000000  0.000000  0.000000   \n",
       "468354  -0.084961 -0.048584  0.055664  ...  0.000000  0.000000  0.000000   \n",
       "1008475 -0.041016  0.052246  0.020020  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "              293       294       295       296       297       298       299  \n",
       "1087812  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "220841   0.157227 -0.045898  0.056641 -0.060547 -0.174805 -0.023071 -0.043457  \n",
       "1271914 -0.039551  0.035156 -0.113281 -0.176758  0.028198 -0.177734 -0.006042  \n",
       "387606   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "349019   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "1531270  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "176086   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1381040  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "468354   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1008475  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[100000 rows x 3000 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = X.input_2.apply(pd.Series)\n",
    "df=pd.DataFrame()\n",
    "for column in df4.columns:\n",
    "    temp=df4.iloc[:,column].apply(pd.Series)\n",
    "    df=pd.concat([df,temp],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train, Y_test = train_test_split(df,Y, test_size=0.2, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "xtrain_np=X_train.to_numpy()\n",
    "x=torch.from_numpy(xtrain_np)\n",
    "\n",
    "ytrain_np=Y_train.to_numpy()\n",
    "y=torch.from_numpy(ytrain_np-1)\n",
    "\n",
    "#testing data\n",
    "xtest_np=X_test.to_numpy()\n",
    "test_x=torch.from_numpy(xtest_np)\n",
    "\n",
    "ytest_np=Y_test.to_numpy()\n",
    "test_y=torch.from_numpy(ytest_np-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.nn.Sequential(\n",
    "    torch.nn.Linear(3000,50),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(50,10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10,5)    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train Accuracy for 0 - tensor(0.2102)\n",
      " train Accuracy for 10 - tensor(0.2211)\n",
      " train Accuracy for 20 - tensor(0.2332)\n",
      " train Accuracy for 30 - tensor(0.2463)\n",
      " train Accuracy for 40 - tensor(0.2625)\n",
      " train Accuracy for 50 - tensor(0.2799)\n",
      " train Accuracy for 60 - tensor(0.2923)\n",
      " train Accuracy for 70 - tensor(0.3007)\n",
      " train Accuracy for 80 - tensor(0.3083)\n",
      " train Accuracy for 90 - tensor(0.3137)\n",
      " train Accuracy for 100 - tensor(0.3202)\n",
      " train Accuracy for 110 - tensor(0.3245)\n",
      " train Accuracy for 120 - tensor(0.3263)\n",
      " train Accuracy for 130 - tensor(0.3288)\n",
      " train Accuracy for 140 - tensor(0.3307)\n"
     ]
    }
   ],
   "source": [
    "model=model.float()\n",
    "for t in range(150):\n",
    "    y_pred=model(x.float())\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if(t%10==0):\n",
    "        pred=model(x.float())\n",
    "        _,predicted = torch.max(pred.data,1)\n",
    "        correct = (predicted == y).sum()\n",
    "        print(\" train Accuracy for\",t,\"-\",correct/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model(test_x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy - 4b: tensor(0.3249)\n"
     ]
    }
   ],
   "source": [
    "_,predicted = torch.max(pred.data,1)\n",
    "correct = (predicted == test_y).sum()\n",
    "print(\"Test Accuracy - 4b:\",correct/len(test_y))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
